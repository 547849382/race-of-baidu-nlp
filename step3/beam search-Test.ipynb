{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本次研讨课内容:\n",
    "   问答摘要内容讲解:\n",
    "    1. beam search 讲解.\n",
    "    2. seq2seq baseline 编写\\训练\\测试.\n",
    "    3. 提交你的第一版成绩.\n",
    "    4. 预测结果分析,模型提升改进点讨论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img-blog.csdnimg.cn/20191212183346344.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0lUX2ZseWluZzYyNQ==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解码是seq2seq模型的常见问题，常用方法有贪心搜索`（Greedy Search）`集束搜索`（Beam Search）`。\n",
    "\n",
    "Decoder根据Encoder的中间语义编码向量c和`<s>`标签得到第一个输出的概率分布`[0.1,0.1,0.3,0.4,0.1]`，选择概率最大的`0.4`，即`moi`。\n",
    "\n",
    "根据隐向量h1h1和moi得到第二个输出的概率分布`[0.1,0.1,0.1,0.1,0.6][0.1,0.1,0.1,0.1,0.6]`，选择概率最大的`0.6`，即`suis`。\n",
    "\n",
    "以此类推，直到遇到`<\\s>`标签，得到最终的序列`moi suis étudiant`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 集束搜索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的贪心搜索只选择了概率最大的一个，而集束搜索则选择了概率最大的前k个。这个k值也叫做集束宽度（Beam Width）。\n",
    "\n",
    "还是以上面的例子作为说明，k值等于2，则集束搜索的过程如下图："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img-blog.csdnimg.cn/20191212183550829.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0lUX2ZseWluZzYyNQ==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到第一个输出的概率分布`[0.1,0.1,0.3,0.4,0.1] [0.1,0.1,0.3,0.4,0.1]`，选择概率最大的前两个，`0.3`和`0.4`，即`Je`和`moi`。\n",
    "\n",
    "然后`Je`和`moi`分别作为`Decoder`的输入，得到两个概率分布，然后再选择概率和最大的前两个序列，`0.3+0.8`和`0.4+0.6`，即`Je suis`和`moi suis`。\n",
    "\n",
    "以此类推，最终可以得到两个序列，即`Je suis étudiant`和`moi suis étudiant`，很明显前者的概率和最大，为`2.2`，所以这个序列是最终得到的结果。\n",
    "\n",
    "集束搜索本质上也是贪心的思想，只不过它考虑了更多的候选搜索空间，因此可以得到更多的翻译结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img-blog.csdn.net/20181011144011354?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2MjM0NjEz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "集束搜索可以认为是维特比算法的贪心形式，在维特比所有中由于利用动态规划导致当字典较大时效率低，而集束搜索使用beam size参数来限制在每一步保留下来的可能性词的数量。集束搜索是在测试阶段为了获得更好准确性而采取的一种策略，在训练阶段无需使用。\n",
    "\n",
    "假设字典为[a,b,c]，beam size选择2，则如下图有：\n",
    "\n",
    "1：在生成第1个词的时候，选择概率最大的2个词，那么当前序列就是a或b\n",
    "\n",
    "2：生成第2个词的时候，我们将当前序列a或b，分别与字典中的所有词进行组合，得到新的6个序列aa ab ac ba bb bc,然后从其中选择2个概率最高的，作为当前序列，即ab或bb\n",
    "\n",
    "3：不断重复这个过程，直到遇到结束符为止。最终输出2个概率最高的序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2020-02-09 19:36:04,173 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/cb/hnw705xd7277l6s881fs2jgm0000gp/T/jieba.cache\n",
      "2020-02-09 19:36:04,180 : DEBUG : Loading model from cache /var/folders/cb/hnw705xd7277l6s881fs2jgm0000gp/T/jieba.cache\n",
      "Loading model cost 2.062 seconds.\n",
      "2020-02-09 19:36:06,239 : DEBUG : Loading model cost 2.062 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "2020-02-09 19:36:06,241 : DEBUG : Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "sys.path.append('/home/roger/kaikeba/03_lecture/code')\n",
    "\n",
    "from utils.wv_loader import load_vocab\n",
    "from utils.data_loader import load_dataset\n",
    "from utils.config import *\n",
    "\n",
    "import numpy as np\n",
    "from utils.gpu_utils import config_gpu\n",
    "config_gpu()\n",
    "import tensorflow as tf\n",
    "from seq2seq_tf2.seq2seq_model import Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, reverse_vocab = load_vocab(vocab_path)\n",
    "vocab_size=len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params[\"vocab_size\"] = vocab_size\n",
    "params[\"embed_size\"] = 500\n",
    "params[\"enc_units\"] = 512\n",
    "params[\"attn_units\"] = 512\n",
    "params[\"dec_units\"] = 512\n",
    "params[\"batch_size\"] = batch_size\n",
    "\n",
    "params[\"beam_size\"]=3\n",
    "\n",
    "params['min_dec_steps']=4 # \n",
    "params['max_dec_steps']=50 # </s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,train_Y,test_X = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31816,   415,   903,     0,   260,   241,     0,   415,    10,\n",
       "          17,     2,    14,     1,     3,   103,    10,    17,     1,\n",
       "           3,   260,   241,     0,   415,     1,     2,    14,     1,\n",
       "           3,    17,   415,   475,     1,     3,    20,     1,     2,\n",
       "          14,     1,     3,   260,   903,     0,   576,   134,     1,\n",
       "           2,     7,    11,     1,     2,    18,   587,     6,     1,\n",
       "           3,    60,   466,  1654,   203,     0,   394,   903,    63,\n",
       "           4,     1,     2,   640,   903,     1,     3,     0, 25980,\n",
       "           1,     2,    14,     1,     3,   431,     0,   329,   221,\n",
       "          24,   903,     0,  1726,  4739,   241,    17,   117,     0,\n",
       "         378,  5231,     0,    33,    58,     7,    81,   329,  2040,\n",
       "           4,     1,     2,    14,     1,     2,    14, 31817, 31818,\n",
       "       31818, 31818, 31818, 31818, 31818, 31818, 31818, 31818, 31818,\n",
       "       31818, 31818, 31818, 31818, 31818, 31818, 31818, 31818, 31818,\n",
       "       31818, 31818, 31818, 31818, 31818, 31818, 31818, 31818, 31818,\n",
       "       31818, 31818, 31818, 31818, 31818, 31818, 31818, 31818, 31818,\n",
       "       31818, 31818, 31818, 31818, 31818, 31818, 31818, 31818, 31818,\n",
       "       31818, 31818, 31818, 31818, 31818, 31818, 31818, 31818, 31818,\n",
       "       31818, 31818, 31818, 31818, 31818, 31818, 31818, 31818, 31818,\n",
       "       31818, 31818, 31818, 31818, 31818, 31818, 31818, 31818, 31818,\n",
       "       31818, 31818, 31818, 31818, 31818, 31818, 31818, 31818, 31818,\n",
       "       31818, 31818, 31818, 31818, 31818, 31818, 31818, 31818, 31818,\n",
       "       31818, 31818])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82873, 200)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 载入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 31820,\n",
       " 'embed_size': 500,\n",
       " 'enc_units': 512,\n",
       " 'attn_units': 512,\n",
       " 'dec_units': 512,\n",
       " 'batch_size': 3,\n",
       " 'beam_size': 3,\n",
       " 'min_dec_steps': 4,\n",
       " 'max_dec_steps': 50}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seq2seq_tf2.seq2seq_model.Seq2Seq at 0x127284dd0>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 读取训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import checkpoint_dir,checkpoint_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/yanqiang/PycharmProjects/kaikeba/lecture_5_2/data/checkpoints/training_checkpoints_mask_loss_dim500_seq'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored\n"
     ]
    }
   ],
   "source": [
    "ckpt = tf.train.Checkpoint(Seq2Seq=model)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_dir, max_to_keep=5)\n",
    "# ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "# print(\"Model restored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img-blog.csdnimg.cn/20191212183346344.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0lUX2ZseWluZzYyNQ==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个类来存储每一步的中间结果\n",
    "class Hypothesis:\n",
    "    \"\"\" Class designed to hold hypothesises throughout the beamSearch decoding \"\"\"\n",
    "    def __init__(self, tokens, log_probs, hidden, attn_dists):\n",
    "        self.tokens = tokens  # list of all the tokens from time 0 to the current time step t\n",
    "        self.log_probs = log_probs  # list of the log probabilities of the tokens of the tokens\n",
    "        self.hidden = hidden  # decoder hidden state after the last token decoding\n",
    "        self.attn_dists = attn_dists  # attention dists of all the tokens\n",
    "        self.abstract = \"\"\n",
    "\n",
    "    def extend(self, token, log_prob, hidden, attn_dist):\n",
    "        \"\"\"Method to extend the current hypothesis by adding the next decoded token and all the informations associated with it\"\"\"\n",
    "        return Hypothesis(tokens=self.tokens + [token],  # we add the decoded token\n",
    "                          log_probs=self.log_probs + [log_prob],  # we add the log prob of the decoded token\n",
    "                          hidden=hidden,  # we update the state\n",
    "                          attn_dists=self.attn_dists + [attn_dist])\n",
    "    @property\n",
    "    def latest_token(self):\n",
    "        return self.tokens[-1]\n",
    "\n",
    "    @property\n",
    "    def tot_log_prob(self):\n",
    "        return sum(self.log_probs)\n",
    "\n",
    "    @property\n",
    "    def avg_log_prob(self):\n",
    "        return self.tot_log_prob / len(self.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化一个对象列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_inp=test_X[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 200)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_output, enc_hidden = model.call_encoder(enc_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 200, 512])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 512])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(512,), dtype=float32, numpy=\n",
       "array([-0.15255082,  0.06849127,  0.02576067, -0.21464548,  0.08560768,\n",
       "        0.12540615, -0.01237227, -0.07497098, -0.0627302 , -0.2219395 ,\n",
       "       -0.3586315 ,  0.03173978,  0.06696814,  0.03594634, -0.10768582,\n",
       "        0.15288863, -0.2617389 , -0.10256704, -0.18194315,  0.17259544,\n",
       "        0.26872373,  0.05511544,  0.33056027,  0.04681689,  0.03308366,\n",
       "        0.02975553,  0.34884924, -0.07119557,  0.00269122, -0.15001512,\n",
       "        0.10215709,  0.12251413,  0.19322753,  0.20117214, -0.20125249,\n",
       "        0.01499808, -0.1921494 ,  0.25185302,  0.0756875 , -0.03805798,\n",
       "       -0.06387903,  0.12379037,  0.0053623 , -0.05715118,  0.11909947,\n",
       "        0.10920532,  0.11579174, -0.00699591,  0.13727206,  0.2802562 ,\n",
       "       -0.11246926,  0.2700776 ,  0.13021748,  0.17622572,  0.21909243,\n",
       "       -0.03193432,  0.22020918, -0.10747288, -0.06599473, -0.08924091,\n",
       "        0.03989922,  0.04054447, -0.25073087, -0.20080546,  0.28185415,\n",
       "       -0.00542684,  0.11774161, -0.01899016,  0.07033592, -0.08823651,\n",
       "        0.18819842, -0.18666032, -0.20621642, -0.04576801,  0.01213269,\n",
       "       -0.16533107,  0.07360062, -0.10818677,  0.14301047,  0.16469067,\n",
       "       -0.10235998, -0.20523441,  0.05867522, -0.02332138,  0.09301154,\n",
       "        0.30056036, -0.14973009, -0.00443567,  0.1140642 ,  0.06165471,\n",
       "       -0.02257682,  0.20824507,  0.10894066,  0.02028284,  0.20915014,\n",
       "        0.08785369,  0.02264285, -0.03741687,  0.1470576 ,  0.07311194,\n",
       "        0.01242066, -0.00303024,  0.08257897, -0.00229721,  0.10103709,\n",
       "        0.08785143,  0.09562372,  0.20136178,  0.01865799, -0.04861511,\n",
       "       -0.17309055, -0.16010043,  0.16383985, -0.02357528,  0.08827582,\n",
       "        0.02628823,  0.0119274 , -0.11431867, -0.00337734, -0.1389778 ,\n",
       "        0.04104049,  0.24232219, -0.07920651,  0.00105615,  0.14927852,\n",
       "       -0.13762695, -0.1347655 , -0.27197576,  0.27183914,  0.1306091 ,\n",
       "        0.001779  ,  0.16836447, -0.05898003, -0.11731704, -0.00355207,\n",
       "       -0.00704208, -0.07443303, -0.4383657 , -0.01769661,  0.278848  ,\n",
       "       -0.11661109,  0.10887885, -0.07860801,  0.08324167,  0.10246649,\n",
       "       -0.08066174, -0.0496188 , -0.02022993, -0.03140361, -0.03673967,\n",
       "        0.19693473,  0.04151224, -0.12852478, -0.00617579, -0.230813  ,\n",
       "        0.31438923, -0.21919727, -0.09992479, -0.08903699, -0.19448349,\n",
       "       -0.03428878, -0.15156955,  0.11413756, -0.05671006,  0.22091094,\n",
       "       -0.23682988,  0.02853238, -0.28172582,  0.10660952, -0.12022489,\n",
       "       -0.06942332, -0.11335423, -0.25458372,  0.1454517 ,  0.20404288,\n",
       "       -0.32915783,  0.03692604, -0.20722708, -0.14006793,  0.04111543,\n",
       "        0.05273184,  0.2802341 , -0.08587795,  0.02134324, -0.070692  ,\n",
       "        0.30900288,  0.1581686 ,  0.25282958, -0.08260943,  0.09169921,\n",
       "       -0.3157143 ,  0.26287287, -0.1514325 ,  0.05618865, -0.25825858,\n",
       "        0.0819294 , -0.1997096 ,  0.36776447,  0.07185606, -0.15092179,\n",
       "        0.19536534,  0.034673  , -0.02290359, -0.04781969, -0.06278603,\n",
       "       -0.39290982, -0.07902707,  0.3056906 ,  0.00169395,  0.01233376,\n",
       "       -0.14570317, -0.23897737, -0.3622861 ,  0.09924737, -0.30395383,\n",
       "       -0.15070798, -0.09974565, -0.15088078,  0.23125556, -0.1792137 ,\n",
       "       -0.02268695,  0.22822994,  0.16977465,  0.06317046,  0.07357764,\n",
       "        0.12254566, -0.10829403, -0.09205458,  0.21803191,  0.16084328,\n",
       "       -0.29248124,  0.17701194, -0.1619651 ,  0.08412811,  0.03219309,\n",
       "       -0.0608067 , -0.20926109, -0.16620153,  0.11196454, -0.14148569,\n",
       "        0.31713367,  0.07133999,  0.2027643 , -0.17993635,  0.02228025,\n",
       "        0.0055878 , -0.09896401,  0.11256565, -0.21307121,  0.36557132,\n",
       "        0.28006577,  0.05560121,  0.05001103,  0.16574016,  0.25556415,\n",
       "       -0.27211714, -0.11335053,  0.00073652, -0.15811759,  0.13735564,\n",
       "       -0.06040768,  0.13148013,  0.28465855,  0.2987684 ,  0.08981371,\n",
       "       -0.07983342,  0.00887136, -0.07387333,  0.15359473,  0.16040203,\n",
       "       -0.34641618,  0.02301655,  0.343974  ,  0.09273776, -0.15871781,\n",
       "       -0.11589558,  0.07523368, -0.00880336,  0.13782819,  0.00351104,\n",
       "       -0.09455815, -0.1494399 ,  0.04416009,  0.08563499, -0.13991833,\n",
       "        0.40898865,  0.00328589,  0.01186489,  0.01833525, -0.23666748,\n",
       "       -0.0955046 , -0.15070203, -0.12931727,  0.14591852,  0.08185355,\n",
       "        0.15429142, -0.21148905,  0.07192734,  0.11874446,  0.09910172,\n",
       "        0.06679887,  0.05828381,  0.0318534 ,  0.08137585, -0.20188457,\n",
       "        0.00794223,  0.07071616,  0.26064056,  0.14224946, -0.01847405,\n",
       "       -0.04694375,  0.25716978,  0.16269764,  0.19563927,  0.42199665,\n",
       "        0.11035059,  0.05856301,  0.13336217, -0.07957514, -0.12923689,\n",
       "       -0.01531795,  0.19082788,  0.22501889, -0.04732311, -0.33450323,\n",
       "        0.13163197,  0.25895682, -0.0509963 ,  0.4209779 , -0.36025113,\n",
       "       -0.10683116, -0.2229136 ,  0.10071091,  0.06134165, -0.2491707 ,\n",
       "        0.09045984, -0.13502544, -0.12807116, -0.0919148 ,  0.05399831,\n",
       "        0.03181089,  0.4489161 , -0.12494136, -0.12236184,  0.01167305,\n",
       "       -0.0295427 ,  0.17078635,  0.11158133,  0.27014518,  0.26076004,\n",
       "       -0.12606508,  0.17837065,  0.06246912, -0.09443858,  0.13899866,\n",
       "        0.09418413, -0.10291958,  0.12398508, -0.14558822, -0.08189775,\n",
       "       -0.2864002 , -0.01301968, -0.256074  , -0.12786031, -0.286973  ,\n",
       "       -0.03567249, -0.12685086, -0.15605444,  0.05310662, -0.22518876,\n",
       "       -0.17925808,  0.2427721 ,  0.2213881 , -0.04428343,  0.48801982,\n",
       "        0.04693007,  0.15752   , -0.06795496, -0.04147592, -0.21575963,\n",
       "       -0.00203847, -0.23044088, -0.21298575, -0.04780135, -0.38105208,\n",
       "        0.11190285, -0.05730342, -0.00719291,  0.07077849,  0.08466569,\n",
       "       -0.35836262, -0.00384131, -0.04688882, -0.09676132,  0.11705647,\n",
       "       -0.09899512, -0.11563288, -0.21575126, -0.07537842, -0.2905649 ,\n",
       "        0.2052007 ,  0.01542677,  0.03362686, -0.14381957,  0.11148573,\n",
       "       -0.28352284,  0.34868222,  0.3604288 , -0.13286951, -0.08713086,\n",
       "        0.25574604,  0.06742263, -0.23972863,  0.07006009, -0.16921785,\n",
       "       -0.03147181,  0.22204545, -0.24374878,  0.11672379,  0.15202409,\n",
       "       -0.16852039,  0.06859508, -0.3156851 ,  0.03488201,  0.14759359,\n",
       "       -0.07088808, -0.20822284, -0.10706595, -0.16763902,  0.32468814,\n",
       "        0.05166193, -0.3734312 , -0.18259919,  0.01850945, -0.28323057,\n",
       "       -0.05128059,  0.0802937 ,  0.08880891, -0.16805485, -0.04220176,\n",
       "        0.11439447, -0.00836199,  0.02706008,  0.01071217,  0.03013179,\n",
       "        0.02856116,  0.1116619 ,  0.18057539, -0.07975787,  0.06007075,\n",
       "        0.3732645 ,  0.03334635, -0.17269965,  0.14496592,  0.03795937,\n",
       "       -0.08528028, -0.04341882,  0.03834542, -0.15359929, -0.05551446,\n",
       "       -0.018366  , -0.07477   , -0.30158412, -0.01750564,  0.07637182,\n",
       "        0.2902224 , -0.17555156,  0.14493033, -0.20678365,  0.19400907,\n",
       "       -0.0133583 , -0.34641933,  0.15281044, -0.04202763, -0.3049336 ,\n",
       "        0.04227328,  0.03000048,  0.04374144, -0.22203708,  0.07553442,\n",
       "       -0.2240918 , -0.05581805, -0.03832652,  0.0452475 , -0.15137672,\n",
       "       -0.25759882,  0.0744964 ,  0.05596338,  0.08756189,  0.03457637,\n",
       "       -0.2343526 ,  0.3426156 , -0.06585666,  0.05303812, -0.13167277,\n",
       "        0.22660905, -0.08683614, -0.11290972, -0.07961328,  0.01167935,\n",
       "        0.04400881, -0.05099613,  0.32643476,  0.12009807, -0.20950086,\n",
       "       -0.0965258 , -0.00133957,  0.19474289, -0.3596087 , -0.00132643,\n",
       "        0.19929901, -0.05970827], dtype=float32)>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_hidden[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps = [Hypothesis(tokens=[vocab['<START>']],\n",
    "                   log_probs=[0.0],\n",
    "                   hidden=enc_hidden[0],\n",
    "                   attn_dists=[],\n",
    "                   ) for _ in range(params['batch_size'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab {a,b,c,d,e,f}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <START> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Hypothesis at 0x1463cbc90>,\n",
       " <__main__.Hypothesis at 0x1462d5890>,\n",
       " <__main__.Hypothesis at 0x1463cead0>]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []  # list to hold the top beam_size hypothesises \n",
    "steps = 0  # initial step 时间步"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取最新tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_tokens = [h.latest_token for h in hyps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31816, 31816, 31816]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 隐藏层状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddens = [h.hidden for h in hyps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hiddens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img-blog.csdn.net/20180414103300419)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单步运行decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_index = vocab['<PAD>']\n",
    "nuk_index = vocab['<UNK>']\n",
    "start_index = vocab['<START>'] # 关注一个\n",
    "stop_index = vocab['<STOP>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31816"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一轮\n",
    "steps=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_onestep(enc_output,dec_input,dec_hidden):\n",
    "    # 单个时间步 运行\n",
    "    preds, dec_hidden, context_vector,attention_weights = model.call_decoder_onestep(dec_input,dec_hidden, enc_output)\n",
    "    # 拿到top k个index 和 概率\n",
    "    top_k_probs, top_k_ids = tf.nn.top_k(tf.squeeze(preds), k=params[\"beam_size\"])\n",
    "    # 计算log概率\n",
    "    top_k_log_probs = tf.math.log(top_k_probs)\n",
    "    # 返回需要保存的中间结果和概率\n",
    "    return preds,dec_hidden,context_vector,attention_weights,top_k_log_probs,top_k_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单次搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算第encoder的输出\n",
    "enc_output, enc_hidden = model.call_encoder(enc_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一个decoder输入 开始标签\n",
    "dec_input = tf.expand_dims(latest_tokens, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一个隐藏层输入\n",
    "dec_hidden = enc_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单步运行\n",
    "preds, dec_hidden, context_vector,attention_weights, top_k_log_probs, top_k_ids = decoder_onestep(enc_output,dec_input,dec_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 31820), dtype=float32, numpy=\n",
       "array([[-0.00768474, -0.0092312 , -0.00109153, ..., -0.01140522,\n",
       "        -0.00739135, -0.00269845],\n",
       "       [-0.00678893, -0.01206916, -0.00251285, ..., -0.01157113,\n",
       "        -0.00701231, -0.00455805],\n",
       "       [-0.00690551, -0.01211246, -0.00217122, ..., -0.01158144,\n",
       "        -0.00649203, -0.00397852]], dtype=float32)>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['beam_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size=3,beam_size=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现阶段全部可能情况\n",
    "all_hyps = [] # 9个\n",
    "# 原有的可能情况数量\n",
    "num_orig_hyps = 1 if steps == 0 else len(hyps)\n",
    "\n",
    "# 遍历添加所有可能结果\n",
    "for i in range(num_orig_hyps):# 1\n",
    "    h, new_hidden, attn_dist = hyps[i], dec_hidden[i], attention_weights[i]\n",
    "    # 分裂 添加 beam size 种可能性\n",
    "    for j in range(params['beam_size']):# 3\n",
    "        # 构造可能的情况\n",
    "        new_hyp = h.extend(token = top_k_ids[i, j].numpy(),\n",
    "                                       log_prob = top_k_log_probs[i, j],\n",
    "                                       hidden = new_hidden,\n",
    "                                       attn_dist = attn_dist)\n",
    "        # 添加可能情况\n",
    "        all_hyps.append(new_hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Hypothesis at 0x12728fa90>,\n",
       " <__main__.Hypothesis at 0x163378390>,\n",
       " <__main__.Hypothesis at 0x163378190>]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_hyps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重置\n",
    "hyps = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照概率来排序\n",
    "sorted_hyps = sorted(all_hyps, key=lambda h: h.avg_log_prob, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选top前beam_size句话 top 3\n",
    "for h in sorted_hyps:\n",
    "    if h.latest_token == stop_index:\n",
    "        # 长度符合预期,遇到句尾,添加到结果集\n",
    "        if steps >= params['min_dec_steps']:\n",
    "            results.append(h)\n",
    "    else:\n",
    "        # 未到结束 ,添加到假设集\n",
    "        hyps.append(h)\n",
    "    \n",
    "    # 如果假设句子正好等于beam_size 或者结果集正好等于beam_size 就不在添加\n",
    "    if len(hyps) == params['beam_size'] or len(results) == params['beam_size']:\n",
    "        break\n",
    "steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results) == 0:\n",
    "    results = hyps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_hyps[0].avg_log_prob\n",
    "# all_hyps[1].avg_log_prob\n",
    "# all_hyps[2].avg_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps_sorted = sorted(results, key=lambda h: h.avg_log_prob, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [p.avg_log_prob for p in hyps_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyp = hyps_sorted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31816, 26703]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyp.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyp.abstract = \" \".join([reverse_vocab[index] for index in best_hyp.tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START> 拉扣'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyp.abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "\n",
    "[1] https://www.tensorflow.org/tutorials/seq2seq\n",
    "\n",
    "[2] https://blog.csdn.net/guolindonggld/article/details/79938567"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# beam search 方法整合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_decode(model,batch,vocab, params):\n",
    "    # 初始化mask\n",
    "    start_index = vocab['<START>']\n",
    "    stop_index = vocab['<STOP>']\n",
    "    \n",
    "    batch_size= params['batch_size']\n",
    "    \n",
    "    # 单步decoder\n",
    "    def decoder_onestep(enc_output,dec_input,dec_hidden):\n",
    "        # 单个时间步 运行\n",
    "        preds, dec_hidden, context_vector,attention_weights = model.call_decoder_onestep(dec_input,dec_hidden, enc_output)\n",
    "        # 拿到top k个index 和 概率\n",
    "        top_k_probs, top_k_ids = tf.nn.top_k(tf.squeeze(preds), k=params[\"beam_size\"])\n",
    "        # 计算log概率\n",
    "        top_k_log_probs = - tf.math.log(top_k_probs)\n",
    "        # 返回需要保存的中间结果和概率\n",
    "        return preds,dec_hidden,context_vector,attention_weights,top_k_log_probs,top_k_ids\n",
    "    \n",
    "    # 计算第encoder的输出\n",
    "    enc_output, enc_hidden = model.call_encoder(batch)\n",
    "    \n",
    "    # 初始化batch size个 假设对象\n",
    "    hyps = [Hypothesis(tokens=[start_index],\n",
    "                   log_probs=[0.0],\n",
    "                   hidden=enc_hidden[0],\n",
    "                   attn_dists=[],\n",
    "                   ) for _ in range(batch_size)]\n",
    "    # 初始化结果集\n",
    "    results = []  # list to hold the top beam_size hypothesises\n",
    "    # 遍历步数\n",
    "    steps = 0  # initial step\n",
    "    \n",
    "    # 第一个隐藏层输入\n",
    "    dec_hidden = enc_hidden\n",
    "    \n",
    "    \n",
    "    # 长度还不够 并且 结果还不够 继续搜索\n",
    "    while steps < params['max_dec_steps'] and len(results) < params['beam_size']:\n",
    "        # 获取最新待使用的token\n",
    "        latest_tokens = [h.latest_token for h in hyps]\n",
    "        # 获取所以隐藏层状态\n",
    "        hiddens = [h.hidden for h in hyps]\n",
    "        # 最新输入\n",
    "        dec_input = tf.expand_dims(latest_tokens, 1)\n",
    "        \n",
    "        # 单步运行decoder 计算需要的值\n",
    "        preds, dec_hidden, context_vector,attention_weights, top_k_log_probs, top_k_ids = decoder_onestep(enc_output,dec_input,dec_hidden)\n",
    "        \n",
    "        # 现阶段全部可能情况\n",
    "        all_hyps = []\n",
    "        # 原有的可能情况数量\n",
    "        num_orig_hyps = 1 if steps == 0 else len(hyps)\n",
    "\n",
    "        # 遍历添加所有可能结果\n",
    "        for i in range(num_orig_hyps):\n",
    "            h, new_hidden, attn_dist = hyps[i], dec_hidden[i], attention_weights[i]\n",
    "            # 分裂 添加 beam size 种可能性\n",
    "            for j in range(params['beam_size']):\n",
    "                if params['batch_size']==1:\n",
    "                    # 构造可能的情况\n",
    "                    new_hyp = h.extend(token = top_k_ids[j].numpy(),\n",
    "                                       log_prob = top_k_log_probs[j],\n",
    "                                       hidden = new_hidden,\n",
    "                                       attn_dist = attn_dist)\n",
    "                else:\n",
    "                    # 构造可能的情况\n",
    "                    new_hyp = h.extend(token = top_k_ids[i, j].numpy(),\n",
    "                                       log_prob = top_k_log_probs[i, j],\n",
    "                                       hidden = new_hidden,\n",
    "                                       attn_dist = attn_dist)\n",
    "                # 添加可能情况\n",
    "                all_hyps.append(new_hyp)\n",
    "        \n",
    "        # 重置\n",
    "        hyps = []\n",
    "        # 按照概率来排序\n",
    "        sorted_hyps = sorted(all_hyps, key=lambda h: h.avg_log_prob, reverse=True)\n",
    "        \n",
    "        # 筛选top前beam_size句话\n",
    "        for h in sorted_hyps:\n",
    "            if h.latest_token == stop_index:\n",
    "                # 长度符合预期,遇到句尾,添加到结果集\n",
    "                if steps >= params['min_dec_steps']:\n",
    "                    results.append(h)\n",
    "            else:\n",
    "                # 未到结束 ,添加到假设集\n",
    "                hyps.append(h)\n",
    "\n",
    "            # 如果假设句子正好等于beam_size 或者结果集正好等于beam_size 就不在添加\n",
    "            if len(hyps) == params['beam_size'] or len(results) == params['beam_size']:\n",
    "                break\n",
    "\n",
    "        steps += 1\n",
    "        \n",
    "    if len(results) == 0:\n",
    "        results = hyps\n",
    "    \n",
    "    hyps_sorted = sorted(results, key=lambda h: h.avg_log_prob, reverse=True)\n",
    "    print(hyps_sorted)\n",
    "    best_hyp = hyps_sorted[0]\n",
    "    best_hyp.abstract = \" \".join([reverse_vocab[index] for index in best_hyp.tokens])\n",
    "    return best_hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取模型\n",
    "model = Seq2Seq(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造数据\n",
    "test_batch=test_X[:3] # beam_size=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 200)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X[:3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 200)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<__main__.Hypothesis object at 0x16339a250>, <__main__.Hypothesis object at 0x14636c090>, <__main__.Hypothesis object at 0x12727c0d0>]\n"
     ]
    }
   ],
   "source": [
    "# 获得最好的语句\n",
    "best_hyp=beam_decode(model,test_batch,vocab, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START> 说真的 会过 新门 学习机 在意 佛 那不修 提个 跟个 科鲁滋 隔行如隔山 到头 超个 大多 10.9 顿顿 科鲁滋 隔行如隔山 提个 驾龄 比亚迪G3 红框 镶 提个 跟个 比亚迪G3 奇怪的是 完怠速 科鲁滋 隔行如隔山 提个 驾龄 比亚迪G3 红框 镶 提个 跟个 比亚迪G3 奇怪的是 完怠速 科鲁滋 隔行如隔山 提个 驾龄 比亚迪G3 红框 镶 提个 跟个 比亚迪G3'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyp.abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    def test_sub():\n",
    "        print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_sub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-2b42ebbf51e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_sub' is not defined"
     ]
    }
   ],
   "source": [
    "test_sub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    def test_sub():\n",
    "        print(1)\n",
    "    test_sub()\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def test_sub():\n",
    "        print(1)\n",
    "def test():\n",
    "    test_sub()\n",
    "test_sub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
