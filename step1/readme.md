## 首先进行数据预处理与词向量训练
### 数据预处理阶段
>>#### 1.关于去掉标点符号，我试验了去除和不去除两种情况，发现不去掉的分数反而高一些
### 构造训练样本batch
>>#### 1.采样时可以过滤高频词
>>#### 2.取样本时，window size 可以取随机数，这样经过多次训练后，可以表示距离目标词近的那些窗口词概率较高
### 词向量训练阶段有两个优化方法
>>#### 1.层次softmax（未采用）
>>> ##### 使用huffman树代替原本的softmax，可以加速训练
>>> ##### 优点：
>>>> 1.时间复杂度从O（V）变为log2（V）
>>>> 2.路径短，参数少
>>>> 3.词频频率越高的词计算越快，路径越短
>>> ##### 缺点：
>>>> 1.有些生僻词但很重要，计算却很慢
>>#### 2.负采样（采用）
>>> #### 每次让一个训练样本仅仅更新一小部分的权重
>>> ##### 优点：
>>>> 1.对高频词效果好
>>>> 2.向量维度低时效果好
>>> ##### 缺点：
>>>> 1.向量维度高时近似误差较大
### 一般在比赛项目中大多数都会使用gensim去训练词向量，所以除了自己学习skip-gram训练数据集外，也使用了gensim去训练
