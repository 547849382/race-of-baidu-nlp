## 首先进行数据预处理与词向量训练
### 数据清理阶段
>>#### 1.关于是否去掉标点符号，我试验了去除和不去除两种情况，发现不去掉的分数反而高一些
### 构造训练样本batch
>>#### 1.采样时可以过滤高频词
>>#### 2.取样本时，window size 可以取随机数，这样经过多次训练后，可以表示距离目标词近的那些窗口词概率较高
### 词向量训练阶段有两个优化方法
>>#### 1.层次softmax（未采用）
>>> ##### 使用huffman树代替原本的softmax，可以加速训练
>>> ##### 优点：
>>>> 1.时间复杂度从O（V）变为log2（V）
>>>> 2.路径短，参数少
>>>> 3.词频频率越高的词计算越快，路径越短
>>> ##### 缺点：
>>>> 1.有些生僻词但很重要，计算却很慢
>>#### 2.负采样（采用）
>>> #### 每次让一个训练样本仅仅更新一小部分的权重
>>> ##### 优点：
>>>> 1.对高频词效果好
>>>> 2.向量维度低时效果好
>>> ##### 缺点：
>>>> 1.向量维度高时近似误差较大
### 使用gensim去训练词向量，对比了Wordvec和fasttext，发现fasttext效果较好
>>>> 由于fasttext采用的是子词训练模型，例如“宝马x5”，fassttext预测出相似的词语为“宝马x3”，“宝马x2”，“宝马x1”等等，但是word2vec预测出的相似词大多是其他车的名称
>>>> tips：在对话样本中，会有“技师说：.....”和“车主说：.......”这些对话，所以同一句话有时在两人说出来的同时会有不同的含义，可以在车主或技师说的话前面加一个特殊符号“#”来进行区分
