{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. attention\n",
    "2. loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bahdanau Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W_s = tf.keras.layers.Dense(units)\n",
    "        self.W_h = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, dec_hidden, enc_output):\n",
    "        # query为上次的GRU隐藏层\n",
    "        # values为编码器的编码结果enc_output\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W_s(enc_output) + self.W_h(hidden_with_time_axis)))\n",
    "       \n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector,attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/e_t.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改造$e^t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W_s = tf.keras.layers.Dense(units)\n",
    "        self.W_h = tf.keras.layers.Dense(units)\n",
    "        self.W_c = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, dec_hidden, enc_output, enc_pad_mask, use_coverage, prev_coverage):\n",
    "        # query 隐藏层\n",
    "        # values为 编码器的编码结果enc_output\n",
    "        hidden_with_time_axis = tf.expand_dims(dec_hidden, 1)\n",
    "        # self.W_s(values)  [batch_sz, max_len, units] self.W_h(hidden_with_time_axis) [batch_sz, 1, units]\n",
    "        # self.W_c(prev_coverage) [batch_sz, max_len, units]  score [batch_sz, max_len, 1]    \n",
    "        score = self.V(tf.nn.tanh(self.W_s(enc_output) + self.W_h(hidden_with_time_axis) + self.W_c(prev_coverage)))\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        # [batch_sz, max_len, enc_units]\n",
    "        context_vector = attention_weights * enc_output\n",
    "        # [batch_sz, enc_units]\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector,attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mask + coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 2 3 4 5 6 0 0 -> 1 1 1 1 1 1 0 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W_s = tf.keras.layers.Dense(units)\n",
    "        self.W_h = tf.keras.layers.Dense(units)\n",
    "        self.W_c = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, dec_hidden, enc_output, enc_pad_mask, use_coverage, prev_coverage):\n",
    "        # query为上次的GRU隐藏层\n",
    "        # values为编码器的编码结果enc_output\n",
    "        # 在seq2seq模型中，St是后面的query向量，而编码过程的隐藏状态hi是values。\n",
    "\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(dec_hidden, 1)\n",
    "\n",
    "        if use_coverage and prev_coverage is not None:\n",
    "            # self.W_s(values) [batch_sz, max_len, units] self.W_h(hidden_with_time_axis) [batch_sz, 1, units]\n",
    "            # self.W_c(prev_coverage) [batch_sz, max_len, units]  score [batch_sz, max_len, 1]\n",
    "            score = self.V(tf.nn.tanh(self.W_s(enc_output) + self.W_h(hidden_with_time_axis) + self.W_c(prev_coverage)))\n",
    "            # attention_weights shape (batch_size, max_len, 1)\n",
    "\n",
    "            mask = tf.cast(enc_pad_mask, dtype=score.dtype)\n",
    "            masked_score = tf.squeeze(score, axis=-1) * mask\n",
    "            masked_score = tf.expand_dims(masked_score, axis=2)\n",
    "\n",
    "            attention_weights = tf.nn.softmax(masked_score, axis=1)\n",
    "            coverage = attention_weights + prev_coverage\n",
    "        else:\n",
    "            # score shape == (batch_size, max_length, 1)\n",
    "            # we get 1 at the last axis because we are applying score to self.V\n",
    "            # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "            # 计算注意力权重值\n",
    "            score = self.V(tf.nn.tanh(\n",
    "                self.W_s(enc_output) + self.W_h(hidden_with_time_axis)))\n",
    "\n",
    "            mask = tf.cast(enc_pad_mask, dtype=score.dtype)\n",
    "            masked_score = tf.squeeze(score, axis=-1) * mask\n",
    "            masked_score = tf.expand_dims(masked_score, axis=2)\n",
    "\n",
    "            attention_weights = tf.nn.softmax(masked_score, axis=1)\n",
    "            # attention_weights = masked_attention(attention_weights)\n",
    "            if use_coverage:\n",
    "                coverage = attention_weights\n",
    "\n",
    "        # attention_weights sha== (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # # 使用注意力权重*编码器输出作为返回值，将来会作为解码器的输入\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector,attention_weights, coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 coverage_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/loss_t.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数\n",
    "def loss_function(real, pred):\n",
    "    pad_mask = tf.math.equal(real, pad_index)\n",
    "    mask = tf.math.logical_not(pad_mask)\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log loss + mask batch loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数\n",
    "def loss_function(real, pred, padding_mask):\n",
    "    loss = 0\n",
    "    for t in range(real.shape[1]):\n",
    "        if padding_mask:\n",
    "            loss_ = loss_object(real[:, t], pred[:, t, :])\n",
    "            mask = tf.cast(padding_mask[:, t], dtype=loss_.dtype)\n",
    "            loss_ *= mask\n",
    "            loss_ = tf.reduce_mean(loss_, axis=0)  # batch-wise\n",
    "            loss += loss_\n",
    "        else:\n",
    "            loss_ = loss_object(real[:, t], pred[:, t, :])\n",
    "            loss_ = tf.reduce_mean(loss_, axis=0)  # batch-wise\n",
    "            loss += loss_\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就是将先前时间步的注意力权重加到一起得到所谓的覆盖向量 $c_t$ (coverage vector)，用先前的注意力权重决策来影响当前注意力权重的决策，这样就避免在同一位置重复，从而避免重复生成文本。计算上，先计算coverage vector $c_t$\n",
    "![](img/c_t.png)\n",
    "+ $c^t$就是一个长度为输入长度的向量\n",
    "+ 第一项是之前时刻输入第一个词attention权重的叠加和\n",
    "\n",
    "+ 加这个参数的目的是为了给attention之前生成词的信息，如果之前生成过这些词那么后续要抑制。抑制通过loss函数加惩罚项实现."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两个地方使用$c_t$:\n",
    "\n",
    "+ 注意力权重的计算过程中 $e^t_i$\n",
    "+ cov_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        pass\n",
    "        \n",
    "    def call(self, dec_hidden, enc_output, enc_pad_mask, use_coverage, prev_coverage):\n",
    "        if use_coverage and prev_coverage is not None:\n",
    "            pass\n",
    "            attention_weights = tf.nn.softmax(score, axis=1)\n",
    "            coverage = attention_weights + prev_coverage\n",
    "        else:    \n",
    "            if use_coverage:\n",
    "                coverage = attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/cobloss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以第一时刻为例，c1表示的是a0，那么a0跟a1当中选择最小的作为loss。\n",
    "\n",
    "如果a0和a1关注的都是同样的分布，那么loss就会比较大，如果他们关注的是不同的分布，因为选择的是两者之中最小的那一个，所以这样的loss会比较小。目的就是想让他每一个时刻关注的分布是不一样的，这样可以避免repeat\n",
    "原文链接：https://blog.csdn.net/ganxiwu9686/article/details/87521054"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<START> 举起 车辆 左 前轮 缸体 上 <STOP> <PAD> <PAD> `\n",
    "\n",
    "`padding_mask`->`[1,1,1,1,1,1,1,0,0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_coverage_loss(attn_dists, coverages, padding_mask):\n",
    "    \"\"\"\n",
    "    Calculates the coverage loss from the attention distributions.\n",
    "      Args:\n",
    "        attn_dists coverages: [max_len_y, batch_sz, max_len_x, 1]\n",
    "        padding_mask: shape (batch_size, max_len_y).\n",
    "      Returns:\n",
    "        coverage_loss: scalar\n",
    "    \"\"\"\n",
    "    cover_losses = []\n",
    "    # transfer attn_dists coverages to [max_len_y, batch_sz, max_len_x]\n",
    "    attn_dists = tf.squeeze(attn_dists, axis=3)\n",
    "    coverages = tf.squeeze(coverages, axis=3)\n",
    "\n",
    "    \n",
    "    for t in range(attn_dists.shape[0]):\n",
    "        cover_loss_ = tf.reduce_sum(tf.minimum(attn_dists[t, :, :], coverages[t, :, :]), axis=-1)  # max_len_x wise\n",
    "        cover_losses.append(cover_loss_)\n",
    "    \n",
    "    # change from[max_len_y, batch_sz] to [batch_sz, max_len_y]\n",
    "    cover_losses = tf.stack(cover_losses, 1)\n",
    "\n",
    "    # cover_loss_ [batch_sz, max_len_y]\n",
    "    mask = tf.cast(padding_mask, dtype=cover_loss_.dtype)\n",
    "    cover_losses *= mask\n",
    "    \n",
    "    # mean loss of each time step and then sum up\n",
    "    loss = tf.reduce_sum(tf.reduce_mean(cover_losses, axis=0))  \n",
    "    tf.print('coverage loss(batch sum):', loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss改变"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/loss_t_coverage.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loss = loss_function(dec_target[:, 1:], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loss = loss_function(dec_target, predictions, padding_mask) + \\\n",
    "                         cov_loss_wt * coverage_loss(attentions, coverages, padding_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
